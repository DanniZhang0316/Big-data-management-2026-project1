{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf52a31",
   "metadata": {},
   "source": [
    "# NYC Taxi ETL - Incremental Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fe62039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9796679",
   "metadata": {},
   "outputs": [],
   "source": [
    "INBOX_PATH = Path(\"data/inbox\")\n",
    "STATE_DIR = Path(\"state\")\n",
    "MANIFEST_PATH = STATE_DIR / \"manifest.json\"\n",
    "\n",
    "STATE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb33216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized: 4.1.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"NYC_Taxi_Incremental_Ingestion\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark session initialized: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "197e2eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest functions defined\n"
     ]
    }
   ],
   "source": [
    "# Manifest management functions\n",
    "def load_manifest():\n",
    "    \"\"\"Load the manifest tracking processed files.\"\"\"\n",
    "    if MANIFEST_PATH.exists():\n",
    "        try:\n",
    "            with open(MANIFEST_PATH, 'r') as f:\n",
    "                manifest = json.load(f)\n",
    "\n",
    "            if \"processed_files\" not in manifest:\n",
    "                manifest[\"processed_files\"] = []\n",
    "            if \"last_run\" not in manifest:\n",
    "                manifest[\"last_run\"] = None\n",
    "            return manifest\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Manifest file corrupted, creating new manifest. Error: {e}\")\n",
    "            return {\"processed_files\": [], \"last_run\": None}\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading manifest: {e}\")\n",
    "            raise\n",
    "    return {\"processed_files\": [], \"last_run\": None}\n",
    "\n",
    "def save_manifest(manifest):\n",
    "    \"\"\"Save the manifest to disk.\"\"\"\n",
    "    try:\n",
    "        manifest[\"last_run\"] = datetime.now().isoformat()\n",
    "        with open(MANIFEST_PATH, 'w') as f:\n",
    "            json.dump(manifest, indent=2, fp=f)\n",
    "        print(f\"Manifest saved: {len(manifest['processed_files'])} files tracked\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving manifest: {e}\")\n",
    "        raise\n",
    "\n",
    "def add_to_manifest(manifest, file_info):\n",
    "    \"\"\"Add a processed file to the manifest.\"\"\"\n",
    "    manifest[\"processed_files\"].append(file_info)\n",
    "\n",
    "print(\"Manifest functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edc8e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet files found in inbox: 3\n",
      "Manifest loaded\n",
      "  - Previously processed: 0 files\n",
      "  - New files found: 2 files\n",
      "    → yellow_tripdata_2025-01.parquet (59,158,238 bytes)\n",
      "    → yellow_tripdata_2025-02.parquet (60,343,086 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Detect new\n",
    "def get_new_files(inbox_path, manifest):\n",
    "    \"\"\"Identify files in inbox that haven't been processed yet.\"\"\"\n",
    "    inbox_path = Path(inbox_path)\n",
    "\n",
    "    if not inbox_path.exists():\n",
    "        raise FileNotFoundError(f\"Inbox path does not exist: {inbox_path}\")\n",
    "\n",
    "    processed_filenames = {f[\"filename\"] for f in manifest[\"processed_files\"]}\n",
    "\n",
    "    all_parquet = sorted(inbox_path.glob(\"*.parquet\"))\n",
    "    print(f\"Parquet files found in inbox: {len(all_parquet)}\")\n",
    "\n",
    "    inbox_files = []\n",
    "    for file in all_parquet:\n",
    "        if \"zone_lookup\" in file.name: # for ignoring the zone lookup\n",
    "            continue\n",
    "\n",
    "        if file.name not in processed_filenames:\n",
    "            file_stat = file.stat()\n",
    "            inbox_files.append({\n",
    "                \"filename\": file.name,\n",
    "                \"path\": str(file),\n",
    "                \"size_bytes\": file_stat.st_size\n",
    "            })\n",
    "\n",
    "    return inbox_files\n",
    "\n",
    "manifest = load_manifest()\n",
    "new_files = get_new_files(INBOX_PATH, manifest)\n",
    "\n",
    "print(\"Manifest loaded\")\n",
    "print(f\"  - Previously processed: {len(manifest['processed_files'])} files\")\n",
    "print(f\"  - New files found: {len(new_files)} files\")\n",
    "if new_files:\n",
    "    for f in new_files:\n",
    "        print(f\"    → {f['filename']} ({f['size_bytes']:,} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0056a206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2 new file(s)...\n",
      "Processing: yellow_tripdata_2025-01.parquet\n",
      "  Rows: 3,475,226\n",
      "  After Cleaning Rows: 2,841,031\n",
      "  After dedup Rows: 2,841,031\n",
      "  Size: 59,158,238 bytes\n",
      "Added to manifest\n",
      "Processing: yellow_tripdata_2025-02.parquet\n",
      "  Rows: 3,577,543\n",
      "  After Cleaning Rows: 2,682,815\n",
      "  After dedup Rows: 2,682,815\n",
      "  Size: 60,343,086 bytes\n",
      "Added to manifest\n",
      "Manifest saved: 2 files tracked\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Process new files and update manifest\n",
    "if len(new_files) == 0:\n",
    "    print(\"No new files to process.\")\n",
    "else:\n",
    "    print(f\"Processing {len(new_files)} new file(s)...\")\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    for file_info in new_files:\n",
    "        print(f\"Processing: {file_info['filename']}\")\n",
    "        \n",
    "        df = spark.read.parquet(file_info['path'])\n",
    "        row_count = df.count()\n",
    "        \n",
    "        # Parse and cast types correctly\n",
    "        df = (\n",
    "            df\n",
    "            .withColumn(\"VendorID\", F.col(\"VendorID\").cast(IntegerType()))\n",
    "            .withColumn(\"tpep_pickup_datetime\", F.to_timestamp(\"tpep_pickup_datetime\"))\n",
    "            .withColumn(\"tpep_dropoff_datetime\", F.to_timestamp(\"tpep_dropoff_datetime\"))\n",
    "            .withColumn(\"passenger_count\", F.col(\"passenger_count\").cast(IntegerType()))\n",
    "            .withColumn(\"trip_distance\", F.col(\"trip_distance\").cast(DoubleType()))\n",
    "            .withColumn(\"RatecodeID\", F.col(\"RatecodeID\").cast(IntegerType()))\n",
    "            .withColumn(\"store_and_fwd_flag\", F.col(\"store_and_fwd_flag\").cast(StringType()))\n",
    "            .withColumn(\"PULocationID\", F.col(\"PULocationID\").cast(IntegerType()))\n",
    "            .withColumn(\"DOLocationID\", F.col(\"DOLocationID\").cast(IntegerType()))\n",
    "            .withColumn(\"payment_type\", F.col(\"payment_type\").cast(IntegerType()))\n",
    "            .withColumn(\"fare_amount\", F.col(\"fare_amount\").cast(DoubleType()))\n",
    "            .withColumn(\"extra\", F.col(\"extra\").cast(DoubleType()))\n",
    "            .withColumn(\"mta_tax\", F.col(\"mta_tax\").cast(DoubleType()))\n",
    "            .withColumn(\"tip_amount\", F.col(\"tip_amount\").cast(DoubleType()))\n",
    "            .withColumn(\"tolls_amount\", F.col(\"tolls_amount\").cast(DoubleType()))\n",
    "            .withColumn(\"improvement_surcharge\", F.col(\"improvement_surcharge\").cast(DoubleType()))\n",
    "            .withColumn(\"total_amount\", F.col(\"total_amount\").cast(DoubleType()))\n",
    "            .withColumn(\"congestion_surcharge\", F.col(\"congestion_surcharge\").cast(DoubleType()))\n",
    "            .withColumn(\"Airport_fee\", F.col(\"Airport_fee\").cast(DoubleType()))\n",
    "            .withColumn(\"cbd_congestion_fee\", F.col(\"cbd_congestion_fee\").cast(DoubleType()))\n",
    "        )\n",
    "        # Apply data cleaning rules\n",
    "        df = df.filter(\n",
    "            F.col(\"tpep_pickup_datetime\").isNotNull() &\n",
    "            F.col(\"tpep_dropoff_datetime\").isNotNull() &\n",
    "            (F.col(\"tpep_dropoff_datetime\") >= F.col(\"tpep_pickup_datetime\")) &\n",
    "            F.col(\"trip_distance\").isNotNull() & (F.col(\"trip_distance\") > 0) &\n",
    "            F.col(\"passenger_count\").isNotNull() & F.col(\"passenger_count\").between(0, 8) &\n",
    "            F.col(\"PULocationID\").isNotNull() & F.col(\"DOLocationID\").isNotNull()\n",
    "        )\n",
    "        # Money related cols should be >=0\n",
    "        money_related_cols = [\n",
    "            \"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\n",
    "            \"improvement_surcharge\",\"congestion_surcharge\",\n",
    "            \"Airport_fee\",\"cbd_congestion_fee\"\n",
    "        ]\n",
    "        \n",
    "        for money in money_related_cols:\n",
    "            df = df.withColumn(money, F.coalesce(F.col(money), F.lit(0.0)))\n",
    "            df = df.filter(F.col(money) >= 0)\n",
    "\n",
    "        # Only total amount can not be null\n",
    "        df = df.filter(\n",
    "            F.col(\"total_amount\").isNotNull() &\n",
    "            (F.col(\"total_amount\") >= 0)\n",
    "        )\n",
    "        new_row_count = df.count()\n",
    "        \n",
    "        # Deduplicate records using a defined key\n",
    "        dedup_key = [\n",
    "            \"VendorID\",\n",
    "            \"tpep_pickup_datetime\",\n",
    "            \"tpep_dropoff_datetime\",\n",
    "            \"PULocationID\",\n",
    "            \"DOLocationID\",\n",
    "            \"trip_distance\",\n",
    "            \"total_amount\",\n",
    "        ]\n",
    "        df = df.dropDuplicates(dedup_key)\n",
    "\n",
    "        after_dedup_row_count = df.count()\n",
    "\n",
    "        print(f\"  Rows: {row_count:,}\")\n",
    "        print(f\"  After Cleaning Rows: {new_row_count:,}\")\n",
    "        print(f\"  After dedup Rows: {after_dedup_row_count:,}\")\n",
    "        print(f\"  Size: {file_info['size_bytes']:,} bytes\")\n",
    "\n",
    "        ingested_at_value = datetime.now().isoformat()\n",
    "\n",
    "        df = (\n",
    "            df\n",
    "            .withColumn(\"trip_duration_minutes\", (F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\")) / 60.0)\n",
    "            .withColumn(\"pickup_date\", F.to_date(\"tpep_pickup_datetime\"))\n",
    "            .withColumn(\"source_file\", F.lit(file_info[\"filename\"]))\n",
    "            .withColumn(\"ingested_at\", F.lit(ingested_at_value))\n",
    "        )\n",
    "\n",
    "        all_dataframes.append(df)\n",
    "\n",
    "        file_metadata = {\n",
    "            \"filename\": file_info['filename'],\n",
    "            \"size_bytes\": file_info['size_bytes'],\n",
    "            \"raw_row_count\": row_count,\n",
    "            \"clean_row_count\": new_row_count,\n",
    "            \"processed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        add_to_manifest(manifest, file_metadata)\n",
    "        \n",
    "        print(f\"Added to manifest\")\n",
    "    \n",
    "    save_manifest(manifest)\n",
    "    \n",
    "    print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5dbedeee-182c-45d4-9cf0-50526044af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriching files\n",
      "Final row count: 5523846\n",
      "Wrote enriched dataset to: data/outbox/trips_enriched.parquet\n"
     ]
    }
   ],
   "source": [
    "OUTBOX_PATH = Path(\"data/outbox\")\n",
    "OUT_PATH = OUTBOX_PATH / \"trips_enriched.parquet\"\n",
    "LOOKUP_PATH = INBOX_PATH / \"taxi_zone_lookup.parquet\"\n",
    "OUTBOX_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if len(new_files) == 0:\n",
    "    print(\"No new cleaned dataframes to enrich\")\n",
    "else:\n",
    "    print(\"Enriching files\")\n",
    "    trips_new = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), \n",
    "                       all_dataframes)\n",
    "\n",
    "    zones = (spark.read.parquet(str(LOOKUP_PATH)).select(\"LocationID\", \"Zone\"))\n",
    "\n",
    "    zones = F.broadcast(zones)\n",
    "\n",
    "    pu = zones.select(\n",
    "        F.col(\"LocationID\").alias(\"PULocationID\"),\n",
    "        F.col(\"Zone\").alias(\"PU_Zone\"),\n",
    "    )\n",
    "    do = zones.select(\n",
    "        F.col(\"LocationID\").alias(\"DOLocationID\"),\n",
    "        F.col(\"Zone\").alias(\"DO_Zone\"),\n",
    "    )\n",
    "\n",
    "    trips_enriched_new = (\n",
    "        trips_new\n",
    "        .join(pu, on=\"PULocationID\", how=\"left\")\n",
    "        .join(do, on=\"DOLocationID\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    required_cols = [\n",
    "        \"tpep_pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\",\n",
    "        \"PULocationID\",\n",
    "        \"DOLocationID\",\n",
    "        \"PU_Zone\",\n",
    "        \"DO_Zone\",\n",
    "        \"passenger_count\",\n",
    "        \"trip_distance\",\n",
    "        \"trip_duration_minutes\",\n",
    "        \"pickup_date\",\n",
    "        \"source_file\",\n",
    "        \"ingested_at\",\n",
    "    ]\n",
    "    trips_enriched_new = trips_enriched_new.select(*required_cols)\n",
    "\n",
    "    # If we already have output data, load it and add the new trips to it\n",
    "    if OUT_PATH.exists():\n",
    "        trips_prev = spark.read.parquet(str(OUT_PATH))\n",
    "        trips_all = trips_prev.unionByName(trips_enriched_new, allowMissingColumns=True)\n",
    "    else:\n",
    "        trips_all = trips_enriched_new\n",
    "    \n",
    "    trips_all = trips_all.cache()\n",
    "    row_c = trips_all.count()\n",
    "    trips_all.write.mode(\"overwrite\").parquet(str(OUT_PATH))\n",
    "    print(\"Final row count:\", row_c)\n",
    "    print(f\"Wrote enriched dataset to: {OUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
