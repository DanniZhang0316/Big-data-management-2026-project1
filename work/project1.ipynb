{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf52a31",
   "metadata": {},
   "source": [
    "# NYC Taxi ETL - Incremental Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe62039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9796679",
   "metadata": {},
   "outputs": [],
   "source": [
    "INBOX_PATH = Path(\"data/inbox\")\n",
    "STATE_DIR = Path(\"state\")\n",
    "MANIFEST_PATH = STATE_DIR / \"manifest.json\"\n",
    "\n",
    "STATE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb33216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized: 4.1.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"NYC_Taxi_Incremental_Ingestion\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark session initialized: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "197e2eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest functions defined\n"
     ]
    }
   ],
   "source": [
    "# Manifest management functions\n",
    "def load_manifest():\n",
    "    \"\"\"Load the manifest tracking processed files.\"\"\"\n",
    "    if MANIFEST_PATH.exists():\n",
    "        try:\n",
    "            with open(MANIFEST_PATH, 'r') as f:\n",
    "                manifest = json.load(f)\n",
    "\n",
    "            if \"processed_files\" not in manifest:\n",
    "                manifest[\"processed_files\"] = []\n",
    "            if \"last_run\" not in manifest:\n",
    "                manifest[\"last_run\"] = None\n",
    "            return manifest\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Manifest file corrupted, creating new manifest. Error: {e}\")\n",
    "            return {\"processed_files\": [], \"last_run\": None}\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading manifest: {e}\")\n",
    "            raise\n",
    "    return {\"processed_files\": [], \"last_run\": None}\n",
    "\n",
    "def save_manifest(manifest):\n",
    "    \"\"\"Save the manifest to disk.\"\"\"\n",
    "    try:\n",
    "        manifest[\"last_run\"] = datetime.now().isoformat()\n",
    "        with open(MANIFEST_PATH, 'w') as f:\n",
    "            json.dump(manifest, indent=2, fp=f)\n",
    "        print(f\"Manifest saved: {len(manifest['processed_files'])} files tracked\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving manifest: {e}\")\n",
    "        raise\n",
    "\n",
    "def add_to_manifest(manifest, file_info):\n",
    "    \"\"\"Add a processed file to the manifest.\"\"\"\n",
    "    manifest[\"processed_files\"].append(file_info)\n",
    "\n",
    "print(\"Manifest functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edc8e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet files found in inbox: 3\n",
      "Manifest loaded\n",
      "  - Previously processed: 2 files\n",
      "  - New files found: 0 files\n"
     ]
    }
   ],
   "source": [
    "# Detect new\n",
    "def get_new_files(inbox_path, manifest):\n",
    "    \"\"\"Identify files in inbox that haven't been processed yet.\"\"\"\n",
    "    inbox_path = Path(inbox_path)\n",
    "\n",
    "    if not inbox_path.exists():\n",
    "        raise FileNotFoundError(f\"Inbox path does not exist: {inbox_path}\")\n",
    "\n",
    "    processed_filenames = {f[\"filename\"] for f in manifest[\"processed_files\"]}\n",
    "\n",
    "    all_parquet = sorted(inbox_path.glob(\"*.parquet\"))\n",
    "    print(f\"Parquet files found in inbox: {len(all_parquet)}\")\n",
    "\n",
    "    inbox_files = []\n",
    "    for file in all_parquet:\n",
    "        if \"zone_lookup\" in file.name: # for ignoring the zone lookup\n",
    "            continue\n",
    "\n",
    "        if file.name not in processed_filenames:\n",
    "            file_stat = file.stat()\n",
    "            inbox_files.append({\n",
    "                \"filename\": file.name,\n",
    "                \"path\": str(file),\n",
    "                \"size_bytes\": file_stat.st_size\n",
    "            })\n",
    "\n",
    "    return inbox_files\n",
    "\n",
    "manifest = load_manifest()\n",
    "new_files = get_new_files(INBOX_PATH, manifest)\n",
    "\n",
    "print(\"Manifest loaded\")\n",
    "print(f\"  - Previously processed: {len(manifest['processed_files'])} files\")\n",
    "print(f\"  - New files found: {len(new_files)} files\")\n",
    "if new_files:\n",
    "    for f in new_files:\n",
    "        print(f\"    → {f['filename']} ({f['size_bytes']:,} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0056a206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new files to process.\n"
     ]
    }
   ],
   "source": [
    "# Process new files and update manifest\n",
    "if len(new_files) == 0:\n",
    "    print(\"No new files to process.\")\n",
    "else:\n",
    "    print(f\"Processing {len(new_files)} new file(s)...\")\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    for file_info in new_files:\n",
    "        print(f\"Processing: {file_info['filename']}\")\n",
    "        \n",
    "        df = spark.read.parquet(file_info['path'])\n",
    "        row_count = df.count()\n",
    "        \n",
    "        # Parse and cast types correctly\n",
    "        df = (\n",
    "            df\n",
    "            .withColumn(\"VendorID\", F.col(\"VendorID\").cast(IntegerType()))\n",
    "            .withColumn(\"tpep_pickup_datetime\", F.to_timestamp(\"tpep_pickup_datetime\"))\n",
    "            .withColumn(\"tpep_dropoff_datetime\", F.to_timestamp(\"tpep_dropoff_datetime\"))\n",
    "            .withColumn(\"passenger_count\", F.col(\"passenger_count\").cast(IntegerType()))\n",
    "            .withColumn(\"trip_distance\", F.col(\"trip_distance\").cast(DoubleType()))\n",
    "            .withColumn(\"RatecodeID\", F.col(\"RatecodeID\").cast(IntegerType()))\n",
    "            .withColumn(\"store_and_fwd_flag\", F.col(\"store_and_fwd_flag\").cast(StringType()))\n",
    "            .withColumn(\"PULocationID\", F.col(\"PULocationID\").cast(IntegerType()))\n",
    "            .withColumn(\"DOLocationID\", F.col(\"DOLocationID\").cast(IntegerType()))\n",
    "            .withColumn(\"payment_type\", F.col(\"payment_type\").cast(IntegerType()))\n",
    "            .withColumn(\"fare_amount\", F.col(\"fare_amount\").cast(DoubleType()))\n",
    "            .withColumn(\"extra\", F.col(\"extra\").cast(DoubleType()))\n",
    "            .withColumn(\"mta_tax\", F.col(\"mta_tax\").cast(DoubleType()))\n",
    "            .withColumn(\"tip_amount\", F.col(\"tip_amount\").cast(DoubleType()))\n",
    "            .withColumn(\"tolls_amount\", F.col(\"tolls_amount\").cast(DoubleType()))\n",
    "            .withColumn(\"improvement_surcharge\", F.col(\"improvement_surcharge\").cast(DoubleType()))\n",
    "            .withColumn(\"total_amount\", F.col(\"total_amount\").cast(DoubleType()))\n",
    "            .withColumn(\"congestion_surcharge\", F.col(\"congestion_surcharge\").cast(DoubleType()))\n",
    "            .withColumn(\"Airport_fee\", F.col(\"Airport_fee\").cast(DoubleType()))\n",
    "            .withColumn(\"cbd_congestion_fee\", F.col(\"cbd_congestion_fee\").cast(DoubleType()))\n",
    "        )\n",
    "        \n",
    "        print(\"\\nBad Row Examples\")\n",
    "        # negative or zero trip_distance\n",
    "        bad_distance = df.filter((F.col(\"trip_distance\").isNull()) | (F.col(\"trip_distance\") <= 0)).limit(3)\n",
    "        bad_distance_count = bad_distance.count()\n",
    "        if bad_distance_count > 0:\n",
    "            print(f\"  Example 1 - Invalid trip_distance (null or ≤0): {bad_distance_count} rows found\")\n",
    "            bad_distance.select(\"trip_distance\", \"tpep_pickup_datetime\", \"fare_amount\").show(3, truncate=False)\n",
    "            print(\"Filtered out (trip_distance must be > 0)\\n\")\n",
    "        \n",
    "        # dropoff before pickup\n",
    "        bad_time = df.filter(\n",
    "            F.col(\"tpep_pickup_datetime\").isNotNull() &\n",
    "            F.col(\"tpep_dropoff_datetime\").isNotNull() &\n",
    "            (F.col(\"tpep_dropoff_datetime\") < F.col(\"tpep_pickup_datetime\"))\n",
    "        ).limit(3)\n",
    "        bad_time_count = bad_time.count()\n",
    "        if bad_time_count > 0:\n",
    "            print(f\"  Example 2 - Time travel (dropoff before pickup): {bad_time_count} rows found\")\n",
    "            bad_time.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"trip_distance\").show(3, truncate=False)\n",
    "            print(\"Filtered out (dropoff must be >= pickup)\\n\")\n",
    "        \n",
    "        # invalid passenger_count\n",
    "        bad_passengers = df.filter(\n",
    "            F.col(\"passenger_count\").isNotNull() & \n",
    "            ~F.col(\"passenger_count\").between(0, 8)\n",
    "        ).limit(3)\n",
    "        bad_passengers_count = bad_passengers.count()\n",
    "        if bad_passengers_count > 0:\n",
    "            print(f\"  Example 3 - Invalid passenger_count (outside 0-8): {bad_passengers_count} rows found\")\n",
    "            bad_passengers.select(\"passenger_count\", \"trip_distance\", \"fare_amount\").show(3, truncate=False)\n",
    "            print(\"Filtered out (passenger_count must be 0-8)\\n\")\n",
    "        \n",
    "        # Apply data cleaning rules\n",
    "        df = df.filter(\n",
    "            F.col(\"tpep_pickup_datetime\").isNotNull() &\n",
    "            F.col(\"tpep_dropoff_datetime\").isNotNull() &\n",
    "            (F.col(\"tpep_dropoff_datetime\") >= F.col(\"tpep_pickup_datetime\")) &\n",
    "            F.col(\"trip_distance\").isNotNull() & (F.col(\"trip_distance\") > 0) &\n",
    "            F.col(\"passenger_count\").isNotNull() & F.col(\"passenger_count\").between(0, 8) &\n",
    "            F.col(\"PULocationID\").isNotNull() & F.col(\"DOLocationID\").isNotNull()\n",
    "        )\n",
    "        # Money related cols should be >=0\n",
    "        money_related_cols = [\n",
    "            \"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\n",
    "            \"improvement_surcharge\",\"congestion_surcharge\",\n",
    "            \"Airport_fee\",\"cbd_congestion_fee\"\n",
    "        ]\n",
    "        \n",
    "        for money in money_related_cols:\n",
    "            df = df.withColumn(money, F.coalesce(F.col(money), F.lit(0.0)))\n",
    "            df = df.filter(F.col(money) >= 0)\n",
    "\n",
    "\n",
    "        df = df.withColumn(\"fare_amount\", F.coalesce(F.col(\"fare_amount\"), F.lit(0.0)))\n",
    "\n",
    "        # Only total amount can not be null\n",
    "        df = df.filter(\n",
    "            F.col(\"total_amount\").isNotNull() &\n",
    "            (F.col(\"total_amount\") >= 0)\n",
    "        )\n",
    "        new_row_count = df.count()\n",
    "        \n",
    "        # Deduplicate records using a defined key\n",
    "        dedup_key = [\n",
    "            \"VendorID\",\n",
    "            \"tpep_pickup_datetime\",\n",
    "            \"tpep_dropoff_datetime\",\n",
    "            \"PULocationID\",\n",
    "            \"DOLocationID\",\n",
    "            \"trip_distance\",\n",
    "            \"total_amount\",\n",
    "        ]\n",
    "        df = df.dropDuplicates(dedup_key)\n",
    "\n",
    "        after_dedup_row_count = df.count()\n",
    "\n",
    "        print(f\"  Rows: {row_count:,}\")\n",
    "        print(f\"  After Cleaning Rows: {new_row_count:,}\")\n",
    "        print(f\"  After dedup Rows: {after_dedup_row_count:,}\")\n",
    "        print(f\"  Size: {file_info['size_bytes']:,} bytes\")\n",
    "\n",
    "        ingested_at_value = datetime.now().isoformat()\n",
    "\n",
    "        df = (\n",
    "            df\n",
    "            .withColumn(\"trip_duration_minutes\", (F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\")) / 60.0)\n",
    "            .withColumn(\"pickup_date\", F.to_date(\"tpep_pickup_datetime\"))\n",
    "            .withColumn(\"source_file\", F.lit(file_info[\"filename\"]))\n",
    "            .withColumn(\"ingested_at\", F.lit(ingested_at_value))\n",
    "        )\n",
    "\n",
    "        all_dataframes.append(df)\n",
    "\n",
    "        file_metadata = {\n",
    "            \"filename\": file_info['filename'],\n",
    "            \"size_bytes\": file_info['size_bytes'],\n",
    "            \"raw_row_count\": row_count,\n",
    "            \"clean_row_count\": new_row_count,\n",
    "            \"after_dedup_row_count\": after_dedup_row_count,\n",
    "            \"processed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        add_to_manifest(manifest, file_metadata)\n",
    "        \n",
    "        print(f\"Added to manifest\")\n",
    "    \n",
    "    save_manifest(manifest)\n",
    "    \n",
    "    print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbedeee-182c-45d4-9cf0-50526044af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new cleaned dataframes to enrich\n"
     ]
    }
   ],
   "source": [
    "OUTBOX_PATH = Path(\"data/outbox\")\n",
    "OUT_PATH = OUTBOX_PATH / \"trips_enriched.parquet\"\n",
    "LOOKUP_PATH = INBOX_PATH / \"taxi_zone_lookup.parquet\"\n",
    "OUTBOX_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if len(new_files) == 0:\n",
    "    print(\"No new cleaned dataframes to enrich\")\n",
    "else:\n",
    "    print(\"Enriching files\")\n",
    "    trips_new = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), \n",
    "                       all_dataframes)\n",
    "\n",
    "    zones = (spark.read.parquet(str(LOOKUP_PATH)).select(\"LocationID\", \"Zone\"))\n",
    "\n",
    "    zones = F.broadcast(zones)\n",
    "\n",
    "    pu = zones.select(\n",
    "        F.col(\"LocationID\").alias(\"PULocationID\"),\n",
    "        F.col(\"Zone\").alias(\"PU_Zone\"),\n",
    "    )\n",
    "    do = zones.select(\n",
    "        F.col(\"LocationID\").alias(\"DOLocationID\"),\n",
    "        F.col(\"Zone\").alias(\"DO_Zone\"),\n",
    "    )\n",
    "\n",
    "    trips_enriched_new = (\n",
    "        trips_new\n",
    "        .join(pu, on=\"PULocationID\", how=\"left\")\n",
    "        .join(do, on=\"DOLocationID\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    required_cols = [\n",
    "        \"tpep_pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\",\n",
    "        \"PULocationID\",\n",
    "        \"DOLocationID\",\n",
    "        \"PU_Zone\",\n",
    "        \"DO_Zone\",\n",
    "        \"passenger_count\",\n",
    "        \"trip_distance\",\n",
    "        \"trip_duration_minutes\",\n",
    "        \"pickup_date\",\n",
    "        \"source_file\",\n",
    "        \"ingested_at\",\n",
    "        \"fare_amount\"\n",
    "    ]\n",
    "    trips_enriched_new = trips_enriched_new.select(*required_cols)\n",
    "\n",
    "    # If we already have output data, load it and add the new trips to it\n",
    "    if OUT_PATH.exists():\n",
    "        trips_prev = spark.read.parquet(str(OUT_PATH))\n",
    "        trips_all = trips_prev.unionByName(trips_enriched_new, allowMissingColumns=True)\n",
    "    else:\n",
    "        trips_all = trips_enriched_new\n",
    "    \n",
    "    trips_all = trips_all.cache()\n",
    "    row_c = trips_all.count()\n",
    "    trips_all.write.mode(\"overwrite\").parquet(str(OUT_PATH))\n",
    "    print(\"Final row count:\", row_c)\n",
    "    print(f\"Wrote enriched dataset to: {OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b61155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Custom Scenario: Flagging Suspicious Trips ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trips_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# CUSTOM SCENARIO: Flag suspicious trips\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Custom Scenario: Flagging Suspicious Trips ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m trips_with_suspicious = \u001b[43mtrips_all\u001b[49m.withColumn(\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mis_suspicious\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     F.when(\n\u001b[32m      7\u001b[39m         (F.col(\u001b[33m\"\u001b[39m\u001b[33mtrip_duration_minutes\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m120\u001b[39m) | \n\u001b[32m      8\u001b[39m         (F.col(\u001b[33m\"\u001b[39m\u001b[33mtrip_distance\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m50\u001b[39m) | \n\u001b[32m      9\u001b[39m         (F.col(\u001b[33m\"\u001b[39m\u001b[33mfare_amount\u001b[39m\u001b[33m\"\u001b[39m) < \u001b[32m0\u001b[39m),\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     11\u001b[39m     ).otherwise(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m total_trips = trips_with_suspicious.count()\n\u001b[32m     15\u001b[39m suspicious_trips = trips_with_suspicious.filter(F.col(\u001b[33m\"\u001b[39m\u001b[33mis_suspicious\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'trips_all' is not defined"
     ]
    }
   ],
   "source": [
    "# CUSTOM SCENARIO: Flag suspicious trips\n",
    "print(\"\\n--- Custom Scenario: Flagging Suspicious Trips ---\")\n",
    "\n",
    "trips_with_suspicious = trips_all.withColumn(\n",
    "    \"is_suspicious\",\n",
    "    F.when(\n",
    "        (F.col(\"trip_duration_minutes\") > 120) | \n",
    "        (F.col(\"trip_distance\") > 50) | \n",
    "        (F.col(\"fare_amount\") < 0),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "total_trips = trips_with_suspicious.count()\n",
    "suspicious_trips = trips_with_suspicious.filter(F.col(\"is_suspicious\") == True)\n",
    "suspicious_count = suspicious_trips.count()\n",
    "\n",
    "print(f\"Total trips: {total_trips:,}\")\n",
    "print(f\"Suspicious trips found: {suspicious_count:,}\")\n",
    "if total_trips > 0:\n",
    "    print(f\"Percentage: {suspicious_count/total_trips*100:.2f}%\")\n",
    "\n",
    "try:\n",
    "    trips_with_suspicious.write.mode(\"overwrite\").parquet(str(OUT_PATH))\n",
    "    print(f\"✓ Main output (with is_suspicious flag) written to: {OUT_PATH}\")\n",
    "    \n",
    "    SUSPICIOUS_PATH = OUTBOX_PATH / \"suspicious_trips.parquet\"\n",
    "    suspicious_trips.write.mode(\"overwrite\").parquet(str(SUSPICIOUS_PATH))\n",
    "    print(f\"✓ Suspicious trips written to: {SUSPICIOUS_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Warning: Could not write files due to Spark connection timeout\")\n",
    "    print(f\"   But data was successfully calculated!\")\n",
    "    print(f\"   Try restarting the kernel and running just this cell again\")\n",
    "\n",
    "print(\"--- Custom Scenario Complete ---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
